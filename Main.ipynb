{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from prophet.python import fbprophet\n",
    "from prophet.python.fbprophet import models\n",
    "from prophet.python.fbprophet import plot\n",
    "from prophet.python.fbprophet import diagnostics\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pkg_resources\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('data/Features data set.csv')\n",
    "features['Date'] = pd.to_datetime(features['Date'], dayfirst=True)\n",
    "sales = pd.read_csv('data/sales data-set.csv')\n",
    "sales['Date'] = pd.to_datetime(sales['Date'], dayfirst=True)\n",
    "stores = pd.read_csv('data/stores data-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_weekly = sales.groupby(['Store', 'Date']).sum().reset_index().merge(stores, on='Store')[['Store', 'Date', 'Weekly_Sales', 'Type', 'Size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leevironty/koodi/kandi/venv/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "for i in features['Store'].unique():\n",
    "    store_mask = features['Store'] == i\n",
    "    store_features = features[store_mask]\n",
    "    cp_flags = store_features['Unemployment'].diff() != 0\n",
    "    store_features.loc[cp_flags, 'u_cp'] = store_features.loc[cp_flags, 'Unemployment']\n",
    "    features.loc[store_mask, 'unemployment_interpolated'] = store_features['u_cp'].interpolate().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features.merge(sales_weekly, on=['Store', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(chunk):\n",
    "    fs = chunk[['Temperature', 'Fuel_Price', 'CPI', 'unemployment_interpolated', 'Weekly_Sales']]\n",
    "    scales = chunk['Size']\n",
    "    return (fs * scales.values[:, np.newaxis]).sum() / scales.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_noise(s, scale=0.05, fii=0.3):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_prophet(df):\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'Date':'ds', 'Weekly_Sales':'y', 'Temperature':'temperature', 'Fuel_Price':'fuel_price', 'CPI':'cpi'})\n",
    "    df['overfit'] = df['y'] / df['y'].max() + np.random.normal(loc=0, scale=0.03)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for t, chunk in data.groupby('Type'):\n",
    "    wm = chunk.groupby('Date').apply(weighted_mean)\n",
    "    datasets[t] = format_for_prophet(wm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>temperature</th>\n",
       "      <th>fuel_price</th>\n",
       "      <th>cpi</th>\n",
       "      <th>unemployment_interpolated</th>\n",
       "      <th>y</th>\n",
       "      <th>overfit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>33.690032</td>\n",
       "      <td>2.703840</td>\n",
       "      <td>170.722140</td>\n",
       "      <td>8.453469</td>\n",
       "      <td>1.576715e+06</td>\n",
       "      <td>0.647904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>32.856791</td>\n",
       "      <td>2.685976</td>\n",
       "      <td>170.819624</td>\n",
       "      <td>8.433678</td>\n",
       "      <td>1.516366e+06</td>\n",
       "      <td>0.624236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>36.120051</td>\n",
       "      <td>2.657191</td>\n",
       "      <td>170.866465</td>\n",
       "      <td>8.413886</td>\n",
       "      <td>1.518252e+06</td>\n",
       "      <td>0.624976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>38.431706</td>\n",
       "      <td>2.678402</td>\n",
       "      <td>170.904866</td>\n",
       "      <td>8.394094</td>\n",
       "      <td>1.370861e+06</td>\n",
       "      <td>0.567173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>42.054062</td>\n",
       "      <td>2.724567</td>\n",
       "      <td>170.943267</td>\n",
       "      <td>8.374303</td>\n",
       "      <td>1.468024e+06</td>\n",
       "      <td>0.605278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>67.947303</td>\n",
       "      <td>3.845498</td>\n",
       "      <td>179.599142</td>\n",
       "      <td>6.835551</td>\n",
       "      <td>1.373897e+06</td>\n",
       "      <td>0.568363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>64.778709</td>\n",
       "      <td>3.822979</td>\n",
       "      <td>179.737298</td>\n",
       "      <td>6.814390</td>\n",
       "      <td>1.515141e+06</td>\n",
       "      <td>0.623756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>56.607945</td>\n",
       "      <td>3.849897</td>\n",
       "      <td>179.875454</td>\n",
       "      <td>6.810223</td>\n",
       "      <td>1.460697e+06</td>\n",
       "      <td>0.602404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>60.217018</td>\n",
       "      <td>3.834338</td>\n",
       "      <td>179.892947</td>\n",
       "      <td>6.806056</td>\n",
       "      <td>1.416175e+06</td>\n",
       "      <td>0.584944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>60.535754</td>\n",
       "      <td>3.749318</td>\n",
       "      <td>179.890331</td>\n",
       "      <td>6.801890</td>\n",
       "      <td>1.431950e+06</td>\n",
       "      <td>0.591130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds  temperature  fuel_price         cpi  \\\n",
       "0   2010-02-05    33.690032    2.703840  170.722140   \n",
       "1   2010-02-12    32.856791    2.685976  170.819624   \n",
       "2   2010-02-19    36.120051    2.657191  170.866465   \n",
       "3   2010-02-26    38.431706    2.678402  170.904866   \n",
       "4   2010-03-05    42.054062    2.724567  170.943267   \n",
       "..         ...          ...         ...         ...   \n",
       "138 2012-09-28    67.947303    3.845498  179.599142   \n",
       "139 2012-10-05    64.778709    3.822979  179.737298   \n",
       "140 2012-10-12    56.607945    3.849897  179.875454   \n",
       "141 2012-10-19    60.217018    3.834338  179.892947   \n",
       "142 2012-10-26    60.535754    3.749318  179.890331   \n",
       "\n",
       "     unemployment_interpolated             y   overfit  \n",
       "0                     8.453469  1.576715e+06  0.647904  \n",
       "1                     8.433678  1.516366e+06  0.624236  \n",
       "2                     8.413886  1.518252e+06  0.624976  \n",
       "3                     8.394094  1.370861e+06  0.567173  \n",
       "4                     8.374303  1.468024e+06  0.605278  \n",
       "..                         ...           ...       ...  \n",
       "138                   6.835551  1.373897e+06  0.568363  \n",
       "139                   6.814390  1.515141e+06  0.623756  \n",
       "140                   6.810223  1.460697e+06  0.602404  \n",
       "141                   6.806056  1.416175e+06  0.584944  \n",
       "142                   6.801890  1.431950e+06  0.591130  \n",
       "\n",
       "[143 rows x 7 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_kwargs = {'adapt_delta': 0.9, 'max_treedepth': 11, 'adapt_kappa': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lppd(model):\n",
    "    return np.log(np.exp(model.params['log_lik']).mean(axis=0)).sum()\n",
    "\n",
    "\n",
    "def aic(model):\n",
    "    k = model.params['beta'].shape[1] + model.params['delta'].shape[1] + 2\n",
    "    return -2*lppd(model) + 2*k\n",
    "\n",
    "\n",
    "def log_lik_bayes_theta(model):\n",
    "    mu = model.predict()['yhat']/model.y_scale\n",
    "    y = model.history['y']/model.y_scale\n",
    "    sigma = model.params['sigma_obs'].mean()\n",
    "    return scipy.stats.norm.logpdf(y, loc=mu, scale=sigma).sum()\n",
    "\n",
    "\n",
    "def dic(model):  # Vois olla nopeempi, nyt log_lik_bayes laskettu kahdesti\n",
    "    return -2*log_lik_bayes_theta(model) + 2*p_dic(model)\n",
    "\n",
    "\n",
    "def dic_alt(model):\n",
    "    return -2*log_lik_bayes_theta(model) + 2*p_dic_alt(model)\n",
    "\n",
    "\n",
    "def waic(model):\n",
    "    #ll = model.params['log_lik']\n",
    "    #pwaic2 = 1/(model.mcmc_samples - 1)*((ll - ll.mean(axis=0))**2).sum(axis=0).sum()\n",
    "    return -2*lppd(model) + 2*p_waic_2(model)\n",
    "\n",
    "def loo_cv(model):\n",
    "    k = model.history.shape[0]\n",
    "    return k_fold_loo_cv(model, k)\n",
    "\n",
    "def k_fold_loo_cv(model, k=10):  # Oletetaan, että residuaalit iid\n",
    "    og_lppd = lppd(model)\n",
    "    data = model.history.sample(frac=1).reset_index(drop=True)  # shuffle\n",
    "    breakpoints = np.linspace(0, data.shape[0], k+1, endpoint=True).astype(int)\n",
    "\n",
    "    #scipy.stats.norm.pdf(p, loc=df['y'].values, scale=m1.params['sigma_obs'][:, np.newaxis])  # TODO: muista skaalaus\n",
    "    def train_test_split(i):  # OK\n",
    "        test = data[breakpoints[i]:breakpoints[i+1]].sort_values(by='t')#.reset_index().drop(columns='index')\n",
    "        train = data.drop(test.index, axis=0).sort_values(by='t').reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "        return (train, test) \n",
    "\n",
    "    res = 0\n",
    "    lppds = []\n",
    "    params = []\n",
    "    for i in range(k):\n",
    "    #for i in tqdm.tqdm(range(k)):\n",
    "        #print(f'CV {i}')\n",
    "        train, test = train_test_split(i)\n",
    "        clean_model = prophet_copy(model)\n",
    "        fit = clean_model.fit(train, control=fit_kwargs)  # Voiko olla ongelma, että t scale muuttuu?\n",
    "        params.append(fit.params.copy())\n",
    "        p = predict_with_samples(fit, test)  \n",
    "        lik = scipy.stats.norm.pdf(p/fit.y_scale, loc=test['y'].values/fit.y_scale, scale=fit.params['sigma_obs'][:, np.newaxis]) # OK\n",
    "        res += np.log(lik.prod(axis=1).mean())  # OK\n",
    "\n",
    "        #p_i = predict_with_samples(fit, model.history)\n",
    "        #b_lik = scipy.stats.norm.pdf(p_i/fit.y_scale, loc=model.history['y'].values/fit.y_scale, scale=fit.params['sigma_obs'][:, np.newaxis])\n",
    "        #lppd_i = np.log(b_lik.prod(axis=1).mean())\n",
    "        #mean_lppd_i += np.log(b_lik.prod(axis=1).mean())/k  # Taitaa olla väärin\n",
    "        lppd_i = 0\n",
    "        for j in range(k):\n",
    "            #print(f'lppd {j}')\n",
    "            train_i, test_i = train_test_split(j)\n",
    "            p_i = predict_with_samples(fit, test_i)\n",
    "            lik_i = scipy.stats.norm.pdf(p_i/fit.y_scale, loc=test_i['y'].values/fit.y_scale, scale=fit.params['sigma_obs'][:, np.newaxis])\n",
    "            lppd_i += np.log(lik_i.prod(axis=1).mean())\n",
    "        lppds.append(lppd_i)\n",
    "\n",
    "    mean_lppd_i = sum(lppds) / k\n",
    "\n",
    "    bias = og_lppd - mean_lppd_i\n",
    "    lppd_bias_corrected = res + bias\n",
    "    eff_params = mean_lppd_i - res\n",
    "    return {'lppd_loo_cv': res, 'lppd_cloo_cv': lppd_bias_corrected, 'bias': bias, 'p_cloo': eff_params, 'fitted_params':params}\n",
    "    \n",
    "\n",
    "def prophet_copy(m):\n",
    "    if m.history is None:\n",
    "        raise Exception('Mallin täytyy olla eka fitattu originaalilla datalla. CV tulee laskea siis vikana')\n",
    "\n",
    "    if m.specified_changepoints:\n",
    "        changepoints = m.changepoints\n",
    "        #if cutoff is not None:\n",
    "        #    # Filter change points '<= cutoff'\n",
    "        #    changepoints = changepoints[changepoints <= cutoff]\n",
    "    else:\n",
    "        changepoints = None\n",
    "\n",
    "    # Auto seasonalities are set to False because they are already set in\n",
    "    # m.seasonalities.\n",
    "    m2 = m.__class__(\n",
    "        growth=m.growth,\n",
    "        n_changepoints=m.n_changepoints,\n",
    "        changepoint_range=m.changepoint_range,\n",
    "        changepoints=changepoints,\n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        holidays=m.holidays,\n",
    "        seasonality_mode=m.seasonality_mode,\n",
    "        seasonality_prior_scale=m.seasonality_prior_scale,\n",
    "        changepoint_prior_scale=m.changepoint_prior_scale,\n",
    "        holidays_prior_scale=m.holidays_prior_scale,\n",
    "        mcmc_samples=m.mcmc_samples,\n",
    "        interval_width=m.interval_width,\n",
    "        uncertainty_samples=m.uncertainty_samples,\n",
    "        stan_backend=m.stan_backend.get_type()\n",
    "    )\n",
    "    #m2.changepoints_t = deepcopy(m.changepoints_t)\n",
    "    m2.extra_regressors = deepcopy(m.extra_regressors)\n",
    "    m2.seasonalities = deepcopy(m.seasonalities)\n",
    "    m2.country_holidays = deepcopy(m.country_holidays)\n",
    "    return m2    \n",
    "    \n",
    "    \n",
    "def predict_with_samples(model, df):\n",
    "    \"\"\"\n",
    "    In: model, prediction df\n",
    "    Out: s*n matrix of predictions\n",
    "    \"\"\"\n",
    "    # Fix scaling issues between different datasets\n",
    "    if 'floor' not in df:\n",
    "        df['floor'] = 0\n",
    "    df['t'] = (df['ds'] - model.start) / model.t_scale\n",
    "    df['y_scaled'] = (df['y'] - df['floor']) / model.y_scale\n",
    "    \n",
    "    trend = predict_trend_with_samples(model, df)\n",
    "    sf = predict_seasonal_components_with_samples(model, df)\n",
    "    return trend * (1 + sf['multiplicative_terms']) + sf['additive_terms']\n",
    "    \n",
    "\n",
    "\n",
    "def predict_seasonal_components_with_samples(model, df): # TODO: make_all antaa train setin kokoisen framen ulos\n",
    "    # Väärin, tutki miks ei toimi\n",
    "    \n",
    "    seasonal_features, _, component_cols, _ = (\n",
    "        model.make_all_seasonality_features(df)\n",
    "    )\n",
    "    X = seasonal_features.values\n",
    "    data = {}\n",
    "    for component in component_cols.columns:\n",
    "        beta_c = model.params['beta'] * component_cols[component].values\n",
    "        comp = (X @ beta_c.T).T\n",
    "        if component in model.component_modes['additive']:\n",
    "            comp *= model.y_scale\n",
    "        data[component] = comp\n",
    "    return data\n",
    "    \n",
    "def predict_trend_with_samples(model, df):  # TODO: implement also for logistic and flat trends\n",
    "    \n",
    "    \n",
    "    #k = np.nanmean(self.params['k'])\n",
    "    #m = np.nanmean(self.params['m'])\n",
    "    #deltas = np.nanmean(self.params['delta'], axis=0)\n",
    "\n",
    "    #t = np.array(df['t'])\n",
    "    #if self.growth == 'linear':\n",
    "    #    trend = self.piecewise_linear(t, deltas, k, m, self.changepoints_t)\n",
    "    #elif self.growth == 'logistic':\n",
    "    #    cap = df['cap_scaled']\n",
    "    #    trend = self.piecewise_logistic(\n",
    "    #        t, cap, deltas, k, m, self.changepoints_t)\n",
    "    #elif self.growth == 'flat':\n",
    "    #    # constant trend\n",
    "    #    trend = self.flat_trend(t, m)\n",
    "        \n",
    "    t_cp = model.changepoints_t\n",
    "    n_cp = t_cp.size\n",
    "    n = df.shape[0]\n",
    "    A = np.zeros([n, n_cp])\n",
    "    for i in range(n):\n",
    "        A[i, :] = t_cp <= df.iloc[i, :]['t']  # TODO: katso että kumpikin on timestamp tai float\n",
    "    \n",
    "    delta = model.params['delta']\n",
    "    k = model.params['k']\n",
    "    m = model.params['m']\n",
    "    \n",
    "    trend = df['t'].values * (k + A @ delta.T).T + (m + A @ (-t_cp * delta).T).T\n",
    "    trend *= model.y_scale\n",
    "\n",
    "    return trend\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_dic(model):\n",
    "    return 2*(log_lik_bayes_theta(model) - model.params['log_lik'].sum(axis=1).mean())\n",
    "              \n",
    "def p_waic_2(model):\n",
    "    ll = model.params['log_lik']\n",
    "    return ((ll - ll.mean(axis=0))**2).var(axis=0, ddof=1).sum()\n",
    "    #return 1/(model.mcmc_samples - 1)*((ll - ll.mean(axis=0))**2).sum(axis=0).sum()  # Tuplacheckattava\n",
    "\n",
    "def p_dic_alt(model):  # Pitäis olla oikein\n",
    "    return model.params['log_lik'].sum(axis=1).var()*2\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(ms, data, no_fit=False, metrics=None):\n",
    "    \"\"\"Lista malleista ja oikean mallinen df (sis. y, ds, x) -> metriikat malleille.\n",
    "    Mallit ei saa olla fitattuja.\"\"\"\n",
    "    res = {}\n",
    "    if metrics is None:\n",
    "        metrics = {'AIC': aic, 'DIC': dic, 'DIC_alt': dic_alt, 'WAIC2': waic, 'p_dic': p_dic, 'p_dic_alt': p_dic_alt, 'p_waic2': p_waic_2, '10-fold_cv': lambda x: k_fold_loo_cv(x, k=10)}\n",
    "    \n",
    "    if no_fit:\n",
    "        fits = ms\n",
    "    else:\n",
    "        fits = [m.fit(data, control=fit_kwargs) for m in ms]\n",
    "    for metric_name, metric in metrics.items():\n",
    "        res[metric_name] = {}\n",
    "        print(f'Evaluating {metric_name}')\n",
    "        for f in fits:\n",
    "            value = metric(f)\n",
    "            res[metric_name][f] = value\n",
    "            if 'cv' not in metric_name:\n",
    "                print(value)\n",
    "            else:\n",
    "                print('Done')\n",
    "        print('')\n",
    "    return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<prophet.python.fbprophet.forecaster.Prophet at 0x7febc22c54d0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = 1000\n",
    "# Initialize models\n",
    "m1 = fbprophet.Prophet(mcmc_samples=samples, seasonality_mode='additive')\n",
    "m2 = fbprophet.Prophet(mcmc_samples=samples, seasonality_mode='additive')\n",
    "m3 = fbprophet.Prophet(mcmc_samples=samples, seasonality_mode='additive')\n",
    "m4 = fbprophet.Prophet(mcmc_samples=samples, yearly_seasonality=50, seasonality_mode='additive')\n",
    "m5 = fbprophet.Prophet(mcmc_samples=samples, yearly_seasonality=False, n_changepoints=1)\n",
    "\n",
    "for r in ['temperature', 'fuel_price', 'cpi', 'unemployment_interpolated']:\n",
    "    m2.add_regressor(r)\n",
    "    \n",
    "m3.add_regressor('overfit', standardize=False)\n",
    "m5.add_regressor('overfit', standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_models = [m1, m2, m3, m4, m5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:97 of 2000 iterations ended with a divergence (4.85 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.9 to remove the divergences.\n",
      "WARNING:pystan:1902 of 2000 iterations saturated the maximum tree depth of 11 (95.1 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 11 to avoid saturation\n",
      "WARNING:pystan:Chain 1: E-BFMI = 0.135\n",
      "WARNING:pystan:Chain 2: E-BFMI = 0.0543\n",
      "WARNING:pystan:Chain 3: E-BFMI = 0.038\n",
      "WARNING:pystan:Chain 4: E-BFMI = 0.0654\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "WARNING:pystan:26 of 2000 iterations saturated the maximum tree depth of 11 (1.3 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 11 to avoid saturation\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:324 of 2000 iterations ended with a divergence (16.2 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.9 to remove the divergences.\n",
      "WARNING:pystan:37 of 2000 iterations saturated the maximum tree depth of 11 (1.85 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 11 to avoid saturation\n",
      "WARNING:pystan:Chain 1: E-BFMI = 0.00604\n",
      "WARNING:pystan:Chain 2: E-BFMI = 0.00904\n",
      "WARNING:pystan:Chain 3: E-BFMI = 0.0107\n",
      "WARNING:pystan:Chain 4: E-BFMI = 0.011\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating aic\n",
      "-393.72201644749134\n",
      "-385.88809448531254\n",
      "-3496.2078220286003\n",
      "-617.7012777970278\n",
      "-3675.247521930617\n",
      "\n",
      "Evaluating dic\n",
      "-439.1831849835243\n",
      "-433.47204099136957\n",
      "-3538.453947685634\n",
      "-724.7614182193716\n",
      "-3681.395050972372\n",
      "\n",
      "Evaluating waic\n",
      "-25.501220956050304\n",
      "-16.812510859771123\n",
      "-3569.0401467923193\n",
      "-3.4814756284580426\n",
      "-3669.7109374293054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = compare_models(used_models, datasets['A'], metrics = {'aic':aic, 'dic':dic, 'waic':waic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10-fold-cv\n",
      "Done\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = compare_models([m1, m2], format_for_prophet(data[data['Store']==2]), no_fit=True, metrics = {'10-fold-cv':lambda x: k_fold_loo_cv(x, k=10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(index=used_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, values in res.items():\n",
    "    for model, value in values.items():\n",
    "        if type(value) == dict:\n",
    "            cols = [c for c in value.keys() if c != 'fitted_params']\n",
    "            for c in cols:\n",
    "                result_df.loc[model, f'{metric_name}_{c}'] = value[c]\n",
    "        else:\n",
    "            result_df.loc[model, metric_name] = value\n",
    "result_df = result_df.rename(index={m1:'m1', m2:'m2', m3:'m3', m4:'m4', m5:'m5'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.round(decimals=2).to_csv('dataset_A_no_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kandi-matplotlib",
   "language": "python",
   "name": "kandi-matplotlib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
